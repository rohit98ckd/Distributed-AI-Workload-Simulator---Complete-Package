{
  "config": {
    "name": "medium_scale_experiment",
    "description": "Medium-scale training with 32 GPUs (4 nodes)",
    "hardware": {
      "gpu_model": "A100",
      "compute_tflops": 312.0,
      "memory_gb": 80.0,
      "network_bandwidth_gbps": 100.0,
      "num_gpus_per_node": 8
    },
    "model": {
      "model_type": "transformer",
      "num_layers": 36,
      "hidden_size": 1280,
      "num_attention_heads": 20,
      "sequence_length": 1024,
      "vocab_size": 50257,
      "total_parameters": null,
      "total_flops": null
    },
    "training": {
      "batch_size_per_gpu": 16,
      "num_iterations": 100,
      "num_nodes": 4,
      "communication_pattern": "ring_allreduce",
      "gradient_accumulation_steps": 1,
      "mixed_precision": false,
      "gradient_checkpointing": false
    },
    "network": {
      "topology_type": "full_mesh",
      "base_latency_us": 5.0,
      "switch_latency_us": 1.0
    },
    "collect_detailed_stats": true,
    "enable_visualization": true,
    "output_directory": "./results"
  },
  "simulation_stats": {
    "total_events": 678400,
    "events_by_type": {
      