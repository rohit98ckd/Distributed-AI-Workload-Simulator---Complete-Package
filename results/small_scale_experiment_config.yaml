collect_detailed_stats: true
description: Small-scale training with 4 GPUs
enable_visualization: true
hardware:
  compute_tflops: 312.0
  gpu_model: A100
  memory_gb: 80.0
  network_bandwidth_gbps: 100.0
  num_gpus_per_node: 8
model:
  hidden_size: 768
  model_type: transformer
  num_attention_heads: 12
  num_layers: 12
  sequence_length: 512
  total_flops: null
  total_parameters: null
  vocab_size: 30522
name: small_scale_experiment
network:
  base_latency_us: 5.0
  switch_latency_us: 1.0
  topology_type: full_mesh
output_directory: ./results
training:
  batch_size_per_gpu: 32
  communication_pattern: ring_allreduce
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  mixed_precision: false
  num_iterations: 100
  num_nodes: 1
